{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textklassifikation mit DraCor\n",
    "\n",
    "[DraCor](https://dracor.org/) ist ein Korpus an Dramen, die mit Hilfe einer umfangreichen [API](https://dracor.org/doc/api) zur Verfügung gestellt werden. In diesem Notebook testen wir, inwiefern sich die Autor:innen der Dramen anhand ihrer Texte identifizieren lassen. Dies ist eine typisches Anwendung der [Stilometrie](https://en.wikipedia.org/wiki/Stylometry).\n",
    "\n",
    "## Korpuserstellung\n",
    "\n",
    "Die ersten beiden Funktionen dienen dazu, ein Korpus an Dramen von DraCor herunterzuladen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "import json \n",
    "\n",
    "dracor_api = \"https://dracor.org/api\"                # API-Endpunkt für DraCor\n",
    "\n",
    "\n",
    "def get_dracor(corpus, play=None):\n",
    "    \"\"\"Lädt entweder Metadaten zum Korpus oder den Text des Stücks.\"\"\"\n",
    "    url = dracor_api + \"/corpora/\" + corpus          # Basis-URL\n",
    "    if play is not None:                             # Stück gewünscht?\n",
    "        url = url + \"/play/\" + play + \"/spoken-text\" # URL für Text des Stückes\n",
    "    with request.urlopen(url) as req:                # Daten herunterladen\n",
    "        text = req.read().decode()                   # Daten einlesen\n",
    "        if play is None:                             # Stück gewünscht?\n",
    "            return json.loads(text)                  # JSON der Korpusmetadaten parsen und zurückgeben\n",
    "        return text                                  # Text des Stückes zurückgeben\n",
    "\n",
    "\n",
    "def get_data(corpus):\n",
    "    \"\"\"Alle Stücke eines Korpus herunterladen.\"\"\"\n",
    "    texts = []                                       # Texte der Stücke\n",
    "    target = []                                      # Autor*innen der Stücke\n",
    "    for drama in get_dracor(corpus)[\"dramas\"]:       # alle Stücke durchlaufen\n",
    "        name = drama[\"name\"]                         # Name des Stücks\n",
    "        authors = drama[\"authors\"]                   # Autor*innen des Stücks\n",
    "        if len(authors) == 1:                        # nur Stücke mit einem/r Autor*in\n",
    "            texts.append(get_dracor(corpus, name))   # Text herunterladen\n",
    "            target.append(authors[0][\"fullname\"])    # Autor*in hinzufügen\n",
    "    return texts, target                             # Texte + Autor*innen als Ergebnis\n",
    "\n",
    "texts, target = get_data(\"ger\")                      # GerDraCor herunterladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textklassifikation\n",
    "\n",
    "Die meisten Verfahren benötigen numerische Daten für die Klassifikation. Daher müssen wir die Texte zunächst transformieren. Die folgende Funktion wandelt daher die gegebenen Daten mittels einer entsprechenden Transformationsklasse um und trainiert und evaluiert dann einen [Naive-Bayes-Klassifikator für multinomiale Daten](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB), der typischerweise gut für die Textklassifikation geeignet ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def texteval(X, Y, vec):\n",
    "    X = vec.fit_transform(X)                                  # Textdaten transformieren\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(X, Y) # in Test- und Trainingsdaten aufteilen\n",
    "    clf = MultinomialNB()                                     # Klassifikator instantiieren\n",
    "    clf.fit(train_X, train_Y)                                 # Model trainieren\n",
    "    return clf.score(test_X, test_Y)                          # Model evaluieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir untersuchen, welchen Einfluss die Art der Texttransformation auf die Klassifikationsgüte hat. \n",
    "\n",
    "### Worthäufigkeiten\n",
    "\n",
    "Beginnen wir mit der einfachsten Möglichkeit: Jedes Dokument wird durch einen Vektor repräsentiert, der für jedes im Korpus vorkommende Wort angibt, wie häufig es im Dokument auftaucht. Das lässt sich mit des [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) umsetzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "for i in range(5):                                            # fünf Durchläufe\n",
    "    print(texteval(texts, target, CountVectorizer()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### häufige Wörter\n",
    "\n",
    "Nur Wörter, die in *mindestens 30%* der Dokumente auftauchen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(texteval(texts, target, CountVectorizer(min_df=0.3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seltene Wörter\n",
    "\n",
    "Nur Wörter, die in *höchstens 30%* der Dokumente auftauchen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(texteval(texts, target, CountVectorizer(max_df=0.3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### häufige Bigramme\n",
    "\n",
    "Nur Bigramme, die in *mindestens 30%* der Dokumente auftauchen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=0.3)\n",
    "\n",
    "for i in range(5):\n",
    "    print(texteval(texts, target, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seltene Bigramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', max_df=0.3)\n",
    "\n",
    "for i in range(5):\n",
    "    print(texteval(texts, target, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "\n",
    "Häufige Wörter sind oft nicht besonders aussagekräftig für ein Dokument, weswegen die Worthäufigkeit häufig mit der Anzahl der Dokumente in Beziehung gesetzt wird, in denen ein Wort vorkommt. Ein übliches Maß dafür ist [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for i in range(5):\n",
    "    print(texteval(texts, target, TfidfVectorizer(min_df=0.3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeichenhäufigkeiten\n",
    "\n",
    "Diese Experimente können wir auf Zeichenebene wiederholen, dafür müssen wir den `CountVectorizer` lediglich einen anderen Analyzer verwenden lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(texteval(texts, target, CountVectorizer(analyzer='char_wb')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### häufige Zeichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(texteval(texts, target, CountVectorizer(analyzer='char_wb', min_df=0.3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seltene Zeichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(texteval(texts, target, CountVectorizer(analyzer='char_wb', max_df=0.3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### häufige Bigramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2), analyzer='char_wb', min_df=0.3)\n",
    "\n",
    "for i in range(5):\n",
    "    print(texteval(texts, target, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seltene Bigramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2), analyzer='char_wb', max_df=0.3)\n",
    "\n",
    "for i in range(5):\n",
    "    print(texteval(texts, target, vec))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
