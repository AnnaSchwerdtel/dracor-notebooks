{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with DraCor\n",
    "\n",
    "\n",
    "[DraCor](https://dracor.org/) is a corpus of plays, which is made available through an extensive [API](https://dracor.org/doc/api).\n",
    "In this notebook we want to test to what extend a play's author can be identified using only the texts they have wrote.\n",
    "This is a typical application of [stylometry](https://en.wikipedia.org/wiki/Stylometry).\n",
    "\n",
    "\n",
    "## Creating the Corpus\n",
    "\n",
    "\n",
    "The first two functions are used to download a corpus of plays from DraCor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "import json \n",
    "\n",
    "dracor_api = \"https://dracor.org/api\"                # DraCor API-endpoint\n",
    "\n",
    "\n",
    "def get_dracor(corpus, play=None):\n",
    "    \"\"\"Loads either corpus metadata or the play's text.\"\"\"\n",
    "    url = dracor_api + \"/corpora/\" + corpus          # base URL\n",
    "    if play is not None:                             # play wanted?\n",
    "        url = url + \"/play/\" + play + \"/spoken-text\" # URL for the play's text\n",
    "    with request.urlopen(url) as req:                # download data\n",
    "        text = req.read().decode()                   # import data\n",
    "        if play is None:                             # play wanted?\n",
    "            return json.loads(text)                  # parse and return JSON of corpus metadata\n",
    "        return text                                  # return the play's text\n",
    "\n",
    "\n",
    "def get_data(corpus):\n",
    "    \"\"\"Download all of one corpus' plays.\"\"\"\n",
    "    texts = []                                       # texts of the plays\n",
    "    target = []                                      # authors of the plays\n",
    "    for drama in get_dracor(corpus)[\"dramas\"]:       # iterate through all plays\n",
    "        name = drama[\"name\"]                         # play title\n",
    "        authors = drama[\"authors\"]                   # play's authors\n",
    "        if len(authors) == 1:                        # keep only plays written by only one author\n",
    "            texts.append(get_dracor(corpus, name))   # download text\n",
    "            target.append(authors[0][\"fullname\"])    # add author\n",
    "    return texts, target                             # return texts and authors (result of this function)\n",
    "\n",
    "texts, target = get_data(\"ger\")                      # download GerDraCor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "Numerical data is required for most classification methods. Therefore we need to transform the texts before we can work with them. The following function changes the given data using a corresponding transformation class. It then trains and evaluates a [Naive Bayes classifier for multinomial models](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB). This classifier is typically well suited for the use in text classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def texteval(X, Y, vec):\n",
    "    X = vec.fit_transform(X)                                  # transform text data\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(X, Y) # split into test and training data\n",
    "    clf = MultinomialNB()                                     # instantiate classificator\n",
    "    clf.fit(train_X, train_Y)                                 # train model\n",
    "    return clf.score(test_X, test_Y)                          # evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to study what influence different types of text transformation have on the quality of the classification.\n",
    "\n",
    "\n",
    "### Word Frequency\n",
    "\n",
    "Let's begin with the simplest option: Every document is represented by a vector. This vector shows the frequency of each word in the corpus within the document. We can do this using the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "for i in range(5):                                            # five iterations\n",
    "    print(texteval(texts, target, CountVectorizer()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequent Words\n",
    "\n",
    "Only words that appear in *at least 30%* of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(texteval(texts, target, CountVectorizer(min_df=0.3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rare Words\n",
    "\n",
    "Only words that appear in *at most 30%* of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(texteval(texts, target, CountVectorizer(max_df=0.3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequent Bigrams\n",
    "\n",
    "Only bigrams that appear in *at least 30%* of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=0.3)\n",
    "\n",
    "for i in range(5):\n",
    "    print(texteval(texts, target, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rare Bigrams\n",
    "\n",
    "Only bigrams that appear in *at most 30%* of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', max_df=0.3)\n",
    "\n",
    "for i in range(5):\n",
    "    print(texteval(texts, target, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Frequent words often are not very meaningful/informative for any given document, therefore the word frequency is often put in relation to the number of documents in which this word appears. A commonly used measure for this is [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for i in range(5):\n",
    "    print(texteval(texts, target, TfidfVectorizer(min_df=0.3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character frequency\n",
    "\n",
    "We can repeat these experiments on the level of individual characters. To do this we simply need to pass a different analyzer to the  `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(texteval(texts, target, CountVectorizer(analyzer='char_wb')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frequent characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(texteval(texts, target, CountVectorizer(analyzer='char_wb', min_df=0.3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rare characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(texteval(texts, target, CountVectorizer(analyzer='char_wb', max_df=0.3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frequent bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2), analyzer='char_wb', min_df=0.3)\n",
    "\n",
    "for i in range(5):\n",
    "    print(texteval(texts, target, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rare bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2), analyzer='char_wb', max_df=0.3)\n",
    "\n",
    "for i in range(5):\n",
    "    print(texteval(texts, target, vec))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
